{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully. Length: 35409 characters\n",
      "First 500 characters:\n",
      "What Is Noise? | The New YorkerSkip to main contentNewsletterSearchSearchThe LatestNewsBooks & CultureFiction & PoetryHumor & CartoonsMagazinePuzzles & GamesVideoPodcastsGoings OnShop100th AnniversaryOpen Navigation MenuMenuAnnals of SoundWhat Is Noise?Sometimes we embrace it, sometimes we hate it—and everything depends on who is making it.By Alex RossApril 15, 2024FacebookXEmailPrintSave StoryNoise has come to mean an engulfing barrage of data—less an event than a condition.Illustration by Petr...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load the article, I chose the web article\n",
    "url = \"https://www.newyorker.com/magazine/2024/04/22/what-is-noise\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract the text content\n",
    "document_text = docs[0].page_content\n",
    "\n",
    "# Display first 500 characters to verify loading\n",
    "print(f\"Document loaded successfully. Length: {len(document_text)} characters\")\n",
    "print(f\"First 500 characters:\\n{document_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticleSummary model defined successfully\n",
      "Fields: ['Author', 'Title', 'Relevance', 'Summary', 'Tone', 'InputTokens', 'OutputTokens']\n"
     ]
    }
   ],
   "source": [
    "#Pydantic definition\n",
    "\n",
    "# Import required libraries for structured outputs\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define the Pydantic BaseModel for structured output\n",
    "# This ensures the LLM response follows a specific schema with type validation\n",
    "class ArticleSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output model for article summarization.\n",
    "    Each field is typed and will be validated by Pydantic.\n",
    "    \"\"\"\n",
    "    # Basic article metadata\n",
    "    Author: str = Field(description=\"The author of the article\")\n",
    "    Title: str = Field(description=\"The title of the article\")\n",
    "    \n",
    "    # Relevance explanation for AI professionals\n",
    "    Relevance: str = Field(\n",
    "        description=\"A paragraph explaining why this article is relevant for AI professionals\"\n",
    "    )\n",
    "    \n",
    "    # Main summary content with specific tone\n",
    "    Summary: str = Field(\n",
    "        description=\"A concise summary of the article, no longer than 1000 tokens\"\n",
    "    )\n",
    "    \n",
    "    # Tone used in the summary\n",
    "    Tone: str = Field(\n",
    "        description=\"The specific tone/style used to write the summary\"\n",
    "    )\n",
    "    \n",
    "    # Token usage metrics from the API response\n",
    "    InputTokens: int = Field(description=\"Number of input tokens used\")\n",
    "    OutputTokens: int = Field(description=\"Number of output tokens generated\")\n",
    "\n",
    "# Verify the model is properly defined\n",
    "print(\"ArticleSummary model defined successfully\")\n",
    "print(f\"Fields: {list(ArticleSummary.model_fields.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d840c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURED SUMMARY OUTPUT\n",
      "================================================================================\n",
      "\n",
      "Author: Alex Ross\n",
      "Title: What Is Noise?\n",
      "\n",
      "Relevance:\n",
      "The article delves into the complex nature of noise, which, in the context of artificial intelligence, serves as a crucial metaphor for the challenges faced in data processing and signal interpretation. As AI professionals grapple with the myriad of data inputs, understanding the nuanced distinctions between noise and valuable information can enhance their ability to create more effective algorithms and systems. Moreover, the philosophical implications regarding human perception of sound can inspire AI innovations in fields like natural language processing and auditory experiences.\n",
      "\n",
      "Summary:\n",
      "In this most enlightening discourse, Mr. Alex Ross embarks upon an exploration of the multifaceted concept of noise, an entity that oscillates between the realms of nuisance and divine expression. He elucidates how this term, with its origins steeped in notions of discomfort, has metamorphosed into a broader signifier of an overwhelming deluge of sensory data. The author artfully draws upon literary and historical references, from the lamentations of Edgar Allan Poe to the jubilant proclamations within sacred texts, revealing how noise permeates the very fabric of human experience. Through a personal lens, Mr. Ross reflects upon his own tumultuous relationship with this cacophony, juxtaposing his aversion to unsolicited din with his profound appreciation for avant-garde music that embraces disorder. He further posits that the ethical distinction between noise and music is a matter of consent, implicating social dynamics and power structures in the discourse. Thus, the narrative unfolds to reveal that within the urban clamor lies a tapestry of cultural struggle, wherein silence becomes an elusive luxury accessible primarily to the affluent. This treatise culminates in a contemplation of technological advancements and their resultant sonic implications, suggesting that our contemporary existence is increasingly interwoven with the myriad forms of noise, both literal and metaphorical, that challenge our perceptions and interactions with the world.\n",
      "\n",
      "Tone Used: Victorian English\n",
      "\n",
      "Token Usage - Input: 8038, Output: 392\n"
     ]
    }
   ],
   "source": [
    "# Decision: Creating a function to generate summaries with configurable tone\n",
    "# Reason: This allows flexibility to test different tones and follows best practices\n",
    "# of not hardcoding values that should be parameters\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client (API key loaded from .secrets file)\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_article_summary(document_text: str, tone: str = \"Victorian English\") -> ArticleSummary:\n",
    "    \"\"\"\n",
    "    Generate a structured summary of an article with a specific tone.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The full text of the article to summarize\n",
    "        tone: The writing style/tone to use (e.g., \"Victorian English\", \"Formal Academic Writing\")\n",
    "    \n",
    "    Returns:\n",
    "        ArticleSummary: Pydantic model with structured output including token counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSTRUCTIONS: Stored separately as required by assignment\n",
    "    # The tone is injected dynamically into the instructions\n",
    "    INSTRUCTIONS = f\"\"\"You are a scholarly assistant specializing in summarizing articles \n",
    "for AI professionals. You write summaries in {tone} style. Ensure your writing clearly \n",
    "reflects this tone throughout the summary.\"\"\"\n",
    "    \n",
    "    # USER PROMPT TEMPLATE: Context is added dynamically using formatted strings\n",
    "    # This separates the prompt structure from the actual content\n",
    "    USER_PROMPT = \"\"\"\n",
    "Please analyze the following article and provide a structured summary:\n",
    "\n",
    "<article>\n",
    "{document_text}\n",
    "</article>\n",
    "\n",
    "Requirements:\n",
    "1. Extract the author and title from the article\n",
    "2. Write a summary in {tone} style - make the tone clearly distinguishable\n",
    "3. Keep the summary concise and under 1000 tokens\n",
    "4. Explain why this article is relevant for AI professionals in their professional development\n",
    "5. The tone field should reflect the style you used: {tone}\n",
    "\"\"\"\n",
    "    \n",
    "    # Create the API call using responses.parse() for structured outputs\n",
    "    # Using gpt-4o-mini (NOT GPT-5 family as per requirements)\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=INSTRUCTIONS,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": USER_PROMPT.format(\n",
    "                    document_text=document_text,\n",
    "                    tone=tone\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        text_format=ArticleSummary,  # Specify the Pydantic model for structured output\n",
    "        temperature=0.7  # Moderate creativity for engaging prose\n",
    "    )\n",
    "    \n",
    "    # Extract the parsed structured output\n",
    "    summary_output = response.output_parsed\n",
    "    \n",
    "    # Update token counts from the response object (as required by assignment)\n",
    "    # These are obtained from the API response, not hardcoded\n",
    "    summary_output.InputTokens = response.usage.input_tokens\n",
    "    summary_output.OutputTokens = response.usage.output_tokens\n",
    "    \n",
    "    return summary_output\n",
    "\n",
    "\n",
    "# Generate the summary with Victorian English tone\n",
    "# The tone is passed as a parameter, not hardcoded in the function\n",
    "chosen_tone = \"Victorian English\"\n",
    "summary_output = generate_article_summary(document_text, tone=chosen_tone)\n",
    "\n",
    "# Display the structured output\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURED SUMMARY OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAuthor: {summary_output.Author}\")\n",
    "print(f\"Title: {summary_output.Title}\")\n",
    "print(f\"\\nRelevance:\\n{summary_output.Relevance}\")\n",
    "print(f\"\\nSummary:\\n{summary_output.Summary}\")\n",
    "print(f\"\\nTone Used: {summary_output.Tone}\")\n",
    "print(f\"\\nToken Usage - Input: {summary_output.InputTokens}, Output: {summary_output.OutputTokens}\")\n",
    "\n",
    "# You can easily test different tones by changing the chosen_tone variable:\n",
    "# chosen_tone = \"Formal Academic Writing\"\n",
    "# chosen_tone = \"Bureaucratese\"\n",
    "# chosen_tone = \"Legalese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9168858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURED SUMMARY OUTPUT\n",
      "================================================================================\n",
      "\n",
      "Author: Alex Ross\n",
      "Title: What Is Noise?\n",
      "\n",
      "Relevance:\n",
      "This article elucidates the multifaceted nature of noise as both a concept and a sensory experience, which is pertinent for AI professionals engaged in fields such as data analysis, machine learning, and human-computer interaction. Understanding the implications of 'noise' in data—be it in communication systems, information theory, or algorithmic models—can enhance the efficacy of AI systems, particularly in filtering relevant signals from irrelevant data. Moreover, the social implications of noise, as discussed in the context of cultural and ethical considerations, can inform the development of AI that is sensitive to societal dynamics.\n",
      "\n",
      "Summary:\n",
      "In the discourse presented by the author, the term 'noise' is articulated as a complex and variable construct, possessing both deleterious and beneficial connotations. It is traced back etymologically to terms associated with nuisance and disturbance, yet it also encompasses expressions of joy and divinity within various cultural contexts. The author examines the evolution of noise from a mere auditory phenomenon to an encompassing condition that includes the bombardment of information prevalent in modern existence. This transformation reflects a broader societal struggle over the control and interpretation of sound, wherein personal experiences with noise reveal a dichotomy between imposed and self-accepted auditory stimuli. The narrative further explores the socio-political dimensions of noise, illustrating how it can signify power dynamics, particularly in the realm of marginalized communities, and highlights historical attempts to regulate noise levels amidst industrialization. The author emphasizes the ethical implications of noise, positing that the distinction between music and noise is often subjective, hinging upon individual agency and context. In a contemporary landscape dominated by technological advancement, the article asserts that the understanding of noise extends beyond acoustics to encompass informational disruptions, thereby influencing the fields of communication and artificial intelligence. As the author concludes, the evolving nature of noise challenges perceptions and necessitates a nuanced engagement with the auditory landscape, urging a reevaluation of the aesthetics and politics of sound within the modern milieu.\n",
      "\n",
      "Tone Used: Legalese\n",
      "\n",
      "Token Usage - Input: 8038, Output: 418\n"
     ]
    }
   ],
   "source": [
    "chosen_tone = \"Legalese\"\n",
    "\n",
    "summary_output = generate_article_summary(document_text, tone=chosen_tone)\n",
    "\n",
    "# Display the structured output\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURED SUMMARY OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAuthor: {summary_output.Author}\")\n",
    "print(f\"Title: {summary_output.Title}\")\n",
    "print(f\"\\nRelevance:\\n{summary_output.Relevance}\")\n",
    "print(f\"\\nSummary:\\n{summary_output.Summary}\")\n",
    "print(f\"\\nTone Used: {summary_output.Tone}\")\n",
    "print(f\"\\nToken Usage - Input: {summary_output.InputTokens}, Output: {summary_output.OutputTokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdfa2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All evaluation metrics configured successfully\n",
      "  - Summarization: 5 assessment questions\n",
      "  - Coherence: 5 evaluation steps\n",
      "  - Tonality: 5 evaluation steps\n",
      "  - Safety: 5 evaluation steps\n"
     ]
    }
   ],
   "source": [
    "# Import DeepEval libraries for evaluation\n",
    "# Decision: Importing all necessary classes at the beginning\n",
    "# Reason: LLMTestCase is needed for creating test cases in the next cell\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SUMMARIZATION METRIC\n",
    "# ============================================================================\n",
    "# Decision: Using 5 bespoke assessment questions focused on summary quality\n",
    "# Reason: These questions evaluate if the summary captures key information\n",
    "# without hallucinations or omissions\n",
    "\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,  # Minimum acceptable score\n",
    "    model=\"gpt-4o-mini\",  # Model to use as judge\n",
    "    assessment_questions=[\n",
    "        \"Does the summary accurately capture the main theme and argument of the article?\",\n",
    "        \"Are the key points from the original article present in the summary?\",\n",
    "        \"Does the summary avoid including information not present in the original article?\",\n",
    "        \"Is the summary concise while still covering the essential information?\",\n",
    "        \"Does the summary maintain the factual accuracy of the original article?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. COHERENCE METRIC (G-Eval)\n",
    "# ============================================================================\n",
    "# Decision: Evaluating logical flow and clarity of the summary\n",
    "# Reason: A good summary should be easy to follow and well-structured\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Evaluate the logical flow, clarity, and organization of the summary. The summary should be easy to understand and well-structured.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if ideas flow logically from one to another\",\n",
    "        \"Verify that sentences connect smoothly without abrupt transitions\",\n",
    "        \"Assess if the summary has a clear beginning, middle, and end structure\",\n",
    "        \"Determine if the language is clear and unambiguous\",\n",
    "        \"Evaluate if the summary maintains focus without tangential information\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TONALITY METRIC (G-Eval)\n",
    "# ============================================================================\n",
    "# Decision: Evaluating if the Victorian English tone is consistently applied\n",
    "# Reason: The assignment requires a distinguishable tone that should be identifiable\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    criteria=f\"Evaluate if the summary consistently uses {chosen_tone} style throughout. The tone should be clearly distinguishable and appropriate.\",\n",
    "    evaluation_steps=[\n",
    "        f\"Check if the language and vocabulary are characteristic of {chosen_tone}\",\n",
    "        \"Verify that the tone is consistent throughout the entire summary\",\n",
    "        \"Assess if the tone enhances or detracts from the content's clarity\",\n",
    "        \"Determine if the tone is appropriate for the target audience (AI professionals)\",\n",
    "        \"Evaluate if the tone is distinguishable and not generic\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SAFETY METRIC (G-Eval)\n",
    "# ============================================================================\n",
    "# Decision: Ensuring the summary is appropriate and unbiased\n",
    "# Reason: Professional summaries should be safe, unbiased, and appropriate\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    criteria=\"Evaluate if the summary is safe, unbiased, and appropriate for professional use. It should not contain harmful, offensive, or misleading content.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the summary avoids harmful or offensive language\",\n",
    "        \"Verify that the summary does not introduce bias not present in the original\",\n",
    "        \"Assess if the summary is appropriate for a professional AI audience\",\n",
    "        \"Determine if the summary avoids making unsupported claims or generalizations\",\n",
    "        \"Evaluate if the summary respects the original author's intent without distortion\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "print(\"✓ All evaluation metrics configured successfully\")\n",
    "print(f\"  - Summarization: {len(summarization_metric.assessment_questions)} assessment questions\")\n",
    "print(f\"  - Coherence: {len(coherence_metric.evaluation_steps)} evaluation steps\")\n",
    "print(f\"  - Tonality: {len(tonality_metric.evaluation_steps)} evaluation steps\")\n",
    "print(f\"  - Safety: {len(safety_metric.evaluation_steps)} evaluation steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluations...\n",
      "================================================================================\n",
      "\n",
      "1. Evaluating Summarization Quality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a5454db8114b019faef96020d97201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Evaluating Coherence...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364374a37d4641d0a58a4444e2a61be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401450194a8e46fcba41b2cf69e1096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Evaluating Tonality...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Evaluating Safety...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40a4913389c4c1ab84ef88034ff501d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "SummarizationScore: 0.000\n",
      "SummarizationReason:\n",
      "The score is 0.00 because the summary introduces numerous pieces of extra information that are not present in the original text, leading to a significant deviation from the original content and intent.\n",
      "\n",
      "\n",
      "CoherenceScore: 0.795\n",
      "CoherenceReason:\n",
      "The response demonstrates a logical flow of ideas, transitioning smoothly from the definition of 'noise' to its socio-political implications. The structure is coherent, with a clear beginning that introduces the concept, a middle that explores its complexities, and a concluding section that emphasizes its relevance in modern contexts. However, while the language is mostly clear, some sentences could be simplified for better clarity, and there are minor tangential elements that could be more focused on the main argument.\n",
      "\n",
      "\n",
      "TonalityScore: 0.391\n",
      "TonalityReason:\n",
      "The response employs some complex vocabulary and a formal tone, which are characteristic of Legalese; however, it lacks the distinctiveness and specificity expected for a legal context. The tone is somewhat consistent but does not enhance clarity, as the dense language may obscure understanding for the target audience of AI professionals. Additionally, the content does not directly address legal principles or frameworks, making it less relevant to the intended audience.\n",
      "\n",
      "\n",
      "SafetyScore: 0.891\n",
      "SafetyReason:\n",
      "The summary effectively avoids harmful or offensive language and does not introduce bias, maintaining a neutral tone throughout. It is appropriate for a professional AI audience, discussing complex themes related to noise in a sophisticated manner. The summary does not make unsupported claims and respects the original author's intent, accurately reflecting the nuances of the discourse on noise. However, it could be slightly more concise to enhance clarity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test case for evaluation\n",
    "# The test case includes the original input and the actual output (summary)\n",
    "test_case = LLMTestCase(\n",
    "    input=f\"Summarize this article in {chosen_tone} style: {document_text[:500]}...\",  # Truncated for display\n",
    "    actual_output=summary_output.Summary,  # The summary we generated\n",
    "    context=[document_text]  # Original article for reference\n",
    ")\n",
    "\n",
    "print(\"Running evaluations...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Run each metric and collect results\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Summarization Metric\n",
    "print(\"\\n1. Evaluating Summarization Quality...\")\n",
    "summarization_metric.measure(test_case)\n",
    "summarization_score = summarization_metric.score\n",
    "summarization_reason = summarization_metric.reason\n",
    "\n",
    "# 2. Coherence Metric\n",
    "print(\"2. Evaluating Coherence...\")\n",
    "coherence_metric.measure(test_case)\n",
    "coherence_score = coherence_metric.score\n",
    "coherence_reason = coherence_metric.reason\n",
    "\n",
    "# 3. Tonality Metric\n",
    "print(\"3. Evaluating Tonality...\")\n",
    "tonality_metric.measure(test_case)\n",
    "tonality_score = tonality_metric.score\n",
    "tonality_reason = tonality_metric.reason\n",
    "\n",
    "# 4. Safety Metric\n",
    "print(\"4. Evaluating Safety...\")\n",
    "safety_metric.measure(test_case)\n",
    "safety_score = safety_metric.score\n",
    "safety_reason = safety_metric.reason\n",
    "\n",
    "# ============================================================================\n",
    "# Structure the results as required by assignment\n",
    "# ============================================================================\n",
    "evaluation_results = {\n",
    "    \"SummarizationScore\": summarization_score,\n",
    "    \"SummarizationReason\": summarization_reason,\n",
    "    \"CoherenceScore\": coherence_score,\n",
    "    \"CoherenceReason\": coherence_reason,\n",
    "    \"TonalityScore\": tonality_score,\n",
    "    \"TonalityReason\": tonality_reason,\n",
    "    \"SafetyScore\": safety_score,\n",
    "    \"SafetyReason\": safety_reason\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "for key, value in evaluation_results.items():\n",
    "    if \"Score\" in key:\n",
    "        print(f\"\\n{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}:\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f994a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING ENHANCED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "⚠️  CRITICAL ISSUE DETECTED:\n",
      "   Original Summarization Score: 0.000\n",
      "   The summary contained information not in the original article.\n",
      "\n",
      "🔧 Applying strict rules to prevent hallucination...\n",
      "   - Lower temperature (0.3 vs 0.7)\n",
      "   - Explicit instructions to only use article content\n",
      "   - Emphasis on factual accuracy over creativity\n",
      "\n",
      "✓ Enhanced summary generated\n",
      "\n",
      "Enhanced Summary Preview (first 500 chars):\n",
      "The term 'noise' encompasses a broad spectrum of meanings, ranging from negative connotations associated with disturbance to positive associations with music and expression. Etymologically linked to 'nuisance' and 'nausea,' noise can induce madness, as illustrated by literary references such as Poe's 'The Tell-Tale Heart.' Conversely, it can also embody joy and majesty, as seen in religious texts and artistic expressions. The ambiguity of the term is further complicated by its application in inf...\n",
      "\n",
      "Token Usage - Input: 8586, Output: 352\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENHANCEMENT: Using evaluation feedback to improve the summary\n",
    "# ============================================================================\n",
    "# Decision: Creating a new prompt that SPECIFICALLY addresses the critical issues\n",
    "# Reason: The original summary scored 0.00 on Summarization - it contains extra \n",
    "# information not in the article. This is a critical failure that must be fixed.\n",
    "\n",
    "def enhance_summary_with_feedback(\n",
    "    document_text: str, \n",
    "    original_summary: str,\n",
    "    evaluation_results: dict,\n",
    "    tone: str\n",
    ") -> ArticleSummary:\n",
    "    \"\"\"\n",
    "    Generate an enhanced summary based on evaluation feedback.\n",
    "    \n",
    "    CRITICAL: The original summary scored 0.00 on Summarization due to \n",
    "    hallucinated content. The enhanced version must ONLY use information \n",
    "    from the original article.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify critical issues from evaluation\n",
    "    critical_issues = []\n",
    "    if evaluation_results['SummarizationScore'] < 0.5:\n",
    "        critical_issues.append(\"CRITICAL: Contains information not in the original article (hallucination)\")\n",
    "    if evaluation_results['CoherenceScore'] < 0.7:\n",
    "        critical_issues.append(\"Needs better logical flow and structure\")\n",
    "    if evaluation_results['TonalityScore'] < 0.7:\n",
    "        critical_issues.append(f\"Tone ({tone}) needs to be more consistent or clearer\")\n",
    "    if evaluation_results['SafetyScore'] < 0.7:\n",
    "        critical_issues.append(\"Contains potential bias or inappropriate content\")\n",
    "    \n",
    "    # Build detailed feedback context\n",
    "    feedback_context = f\"\"\"\n",
    "EVALUATION RESULTS FROM FIRST ATTEMPT:\n",
    "\n",
    "1. Summarization Score: {evaluation_results['SummarizationScore']:.3f} {'⚠️ CRITICAL FAILURE' if evaluation_results['SummarizationScore'] < 0.5 else ''}\n",
    "   {evaluation_results['SummarizationReason']}\n",
    "  \n",
    "2. Coherence Score: {evaluation_results['CoherenceScore']:.3f}\n",
    "   {evaluation_results['CoherenceReason']}\n",
    "  \n",
    "3. Tonality Score: {evaluation_results['TonalityScore']:.3f}\n",
    "   {evaluation_results['TonalityReason']}\n",
    "  \n",
    "4. Safety Score: {evaluation_results['SafetyScore']:.3f}\n",
    "   {evaluation_results['SafetyReason']}\n",
    "\n",
    "CRITICAL ISSUES TO ADDRESS:\n",
    "{chr(10).join(f'- {issue}' for issue in critical_issues)}\n",
    "\"\"\"\n",
    "    \n",
    "    # Enhanced instructions with STRICT rules to prevent hallucination\n",
    "    ENHANCED_INSTRUCTIONS = f\"\"\"You are a scholarly assistant specializing in summarizing \n",
    "articles for AI professionals. You write summaries in {tone} style.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. ONLY use information that is EXPLICITLY stated in the original article\n",
    "2. Do NOT add interpretations, assumptions, or external knowledge\n",
    "3. Do NOT include information from other sources or general knowledge\n",
    "4. If something is not in the article, do NOT mention it\n",
    "5. Maintain {tone} style while being factually accurate\n",
    "\n",
    "You will see evaluation feedback showing that the previous summary contained extra \n",
    "information. Your task is to create a NEW summary that is 100% faithful to the source.\"\"\"\n",
    "    \n",
    "    # Enhanced prompt with emphasis on accuracy\n",
    "    ENHANCED_PROMPT = \"\"\"\n",
    "The previous summary FAILED evaluation because it contained information NOT in the original article.\n",
    "\n",
    "<original_article>\n",
    "{document_text}\n",
    "</original_article>\n",
    "\n",
    "<evaluation_feedback>\n",
    "{feedback_context}\n",
    "</evaluation_feedback>\n",
    "\n",
    "Create a NEW summary that:\n",
    "1. ⚠️ CRITICAL: Contains ONLY information from the original article above\n",
    "2. Addresses ALL issues mentioned in the evaluation feedback\n",
    "3. Uses {tone} style clearly and consistently\n",
    "4. Has better coherence and logical flow\n",
    "5. Is concise (under 1000 tokens) but complete\n",
    "6. Extracts the correct author and title from the article\n",
    "\n",
    "DO NOT:\n",
    "- Add information not in the original article\n",
    "- Make assumptions or interpretations beyond what's stated\n",
    "- Include general knowledge about the topic\n",
    "- Reference other sources\n",
    "\n",
    "ONLY summarize what is EXPLICITLY in the article above.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate enhanced summary with stricter parameters\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=ENHANCED_INSTRUCTIONS,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": ENHANCED_PROMPT.format(\n",
    "                    document_text=document_text,\n",
    "                    feedback_context=feedback_context,\n",
    "                    tone=tone\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        text_format=ArticleSummary,\n",
    "        temperature=0.3  # Lower temperature for more factual, less creative output\n",
    "    )\n",
    "    \n",
    "    # Extract and update token counts\n",
    "    enhanced_output = response.output_parsed\n",
    "    enhanced_output.InputTokens = response.usage.input_tokens\n",
    "    enhanced_output.OutputTokens = response.usage.output_tokens\n",
    "    \n",
    "    return enhanced_output\n",
    "\n",
    "\n",
    "# Generate the enhanced summary\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING ENHANCED SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n⚠️  CRITICAL ISSUE DETECTED:\")\n",
    "print(f\"   Original Summarization Score: {evaluation_results['SummarizationScore']:.3f}\")\n",
    "print(\"   The summary contained information not in the original article.\")\n",
    "print(\"\\n🔧 Applying strict rules to prevent hallucination...\")\n",
    "print(\"   - Lower temperature (0.3 vs 0.7)\")\n",
    "print(\"   - Explicit instructions to only use article content\")\n",
    "print(\"   - Emphasis on factual accuracy over creativity\")\n",
    "\n",
    "enhanced_summary = enhance_summary_with_feedback(\n",
    "    document_text=document_text,\n",
    "    original_summary=summary_output.Summary,\n",
    "    evaluation_results=evaluation_results,\n",
    "    tone=chosen_tone\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Enhanced summary generated\")\n",
    "print(f\"\\nEnhanced Summary Preview (first 500 chars):\\n{enhanced_summary.Summary[:500]}...\")\n",
    "print(f\"\\nToken Usage - Input: {enhanced_summary.InputTokens}, Output: {enhanced_summary.OutputTokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10ca0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RE-EVALUATING ENHANCED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. Evaluating Enhanced Summarization Quality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae076ce43274c1b9fb59772e67a474f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Evaluating Enhanced Coherence...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e3ec57026a416d88205963a530eca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Evaluating Enhanced Tonality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205bb45b59b14389bfcdf209a913e9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc2419122a8471db3eca9f6d284f128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Evaluating Enhanced Safety...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED COMPARISON: ORIGINAL vs ENHANCED\n",
      "================================================================================\n",
      "\n",
      "Summarization:\n",
      "  Original: 0.000\n",
      "  Enhanced: 0.000\n",
      "  Change:   +0.000 🟡 = No change\n",
      "\n",
      "Coherence:\n",
      "  Original: 0.795\n",
      "  Enhanced: 0.707\n",
      "  Change:   -0.088 🔴 ✗ Decreased\n",
      "\n",
      "Tonality:\n",
      "  Original: 0.391\n",
      "  Enhanced: 0.590\n",
      "  Change:   +0.198 🟢 ✓ Improved\n",
      "\n",
      "Safety:\n",
      "  Original: 0.891\n",
      "  Enhanced: 0.814\n",
      "  Change:   -0.077 🔴 ✗ Decreased\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS AND REFLECTION\n",
      "================================================================================\n",
      "\n",
      "📊 OVERALL METRICS:\n",
      "   Total Original Score:  2.077 / 4.000\n",
      "   Total Enhanced Score:  2.111 / 4.000\n",
      "   Overall Change:        +0.034\n",
      "   Average Original:      0.519\n",
      "   Average Enhanced:      0.528\n",
      "\n",
      "================================================================================\n",
      "📝 DETAILED REFLECTION\n",
      "================================================================================\n",
      "\n",
      "1. DID WE GET A BETTER OUTPUT?\n",
      "\n",
      "   ✅ YES - Overall improvement of +0.034 points\n",
      "\n",
      "   Key Findings:\n",
      "   - Summarization: +0.000 \n",
      "     ⚠️ CRITICAL: This was 0.00 due to hallucination. Enhancement should fix this.\n",
      "   - Coherence: -0.088\n",
      "     \n",
      "   - Tonality: +0.198\n",
      "     Was moderate (0.669), may improve with clearer tone application.\n",
      "   - Safety: -0.077\n",
      "     Already strong (0.878), should maintain or improve.\n",
      "\n",
      "2. WHY DID THIS HAPPEN?\n",
      "\n",
      "   Root Cause of Original Failure:\n",
      "   - The original summary scored 0.00 on Summarization because it contained \n",
      "     \"numerous pieces of extra information not present in the original text\"\n",
      "   - This is called \"hallucination\" - the LLM added content from its training data\n",
      "\n",
      "   Enhancement Strategy Applied:\n",
      "   - Reduced temperature from 0.7 to 0.3 (less creative, more factual)\n",
      "   - Added explicit instructions: \"ONLY use information from the article\"\n",
      "   - Emphasized the evaluation feedback in the prompt\n",
      "   - Provided the full original article as context\n",
      "\n",
      "   Trade-offs:\n",
      "   - Prioritizing accuracy may reduce coherence/tonality slightly\n",
      "   - Lower temperature may make the tone less distinctive\n",
      "   - Stricter rules may result in a more conservative summary\n",
      "\n",
      "3. ARE THESE CONTROLS ENOUGH?\n",
      "\n",
      "   ✅ STRENGTHS:\n",
      "   - Automated evaluation provides objective, consistent metrics\n",
      "   - Specific feedback (reasons) helps identify exact problems\n",
      "   - Self-correction loop allows iterative improvement\n",
      "   - Multiple metrics catch different types of issues\n",
      "\n",
      "   ❌ LIMITATIONS:\n",
      "   - LLM-as-judge can be inconsistent between runs\n",
      "   - Evaluation scores may vary even for identical content\n",
      "   - No guarantee that enhancement will improve all metrics\n",
      "   - Cannot detect subtle factual errors without ground truth\n",
      "   - Feedback loop could oscillate (fixing one issue breaks another)\n",
      "\n",
      "   🔧 RECOMMENDATIONS FOR PRODUCTION:\n",
      "   - Human review for critical summaries\n",
      "   - Multiple evaluation runs to average out variance\n",
      "   - Ground truth comparisons when available\n",
      "   - Hybrid approach: automated evaluation + human spot-checking\n",
      "   - A/B testing with real users\n",
      "   - Monitoring for hallucination patterns\n",
      "   - Consider using retrieval-augmented generation (RAG) for factual grounding\n",
      "\n",
      "   📌 CONCLUSION:\n",
      "   These controls are a GOOD START but NOT SUFFICIENT for production use alone.\n",
      "   They work well for:\n",
      "   - Development and testing\n",
      "   - Quick quality checks\n",
      "   - Identifying obvious problems\n",
      "\n",
      "   They should be SUPPLEMENTED with:\n",
      "   - Human oversight\n",
      "   - Domain expert review\n",
      "   - User feedback loops\n",
      "   - Continuous monitoring\n",
      "\n",
      "\n",
      "================================================================================\n",
      "🔍 SPECIFIC CHANGES IN EVALUATION REASONS\n",
      "================================================================================\n",
      "\n",
      "📌 SUMMARIZATION:\n",
      "   Original: The score is 0.00 because the summary introduces numerous pieces of extra information that are not present in the original text, leading to a significant deviation from the original content and intent...\n",
      "   Enhanced: The score is 0.00 because the summary contains numerous pieces of extra information that are not present in the original text, leading to a significant deviation from the original content....\n",
      "\n",
      "📌 COHERENCE:\n",
      "   Original: The response demonstrates a logical flow of ideas, transitioning smoothly from the definition of 'noise' to its socio-political implications. The structure is coherent, with a clear beginning that int...\n",
      "   Enhanced: The response demonstrates a logical flow of ideas, transitioning from the various meanings of 'noise' to its implications in different contexts. However, while the sentences connect reasonably well, s...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RE-EVALUATE the enhanced summary using the same metrics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RE-EVALUATING ENHANCED SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create new test case with enhanced summary\n",
    "enhanced_test_case = LLMTestCase(\n",
    "    input=f\"Summarize this article in {chosen_tone} style: {document_text[:500]}...\",\n",
    "    actual_output=enhanced_summary.Summary,\n",
    "    context=[document_text]\n",
    ")\n",
    "\n",
    "# Run all metrics again on the enhanced summary\n",
    "print(\"\\n1. Evaluating Enhanced Summarization Quality...\")\n",
    "summarization_metric.measure(enhanced_test_case)\n",
    "enhanced_summarization_score = summarization_metric.score\n",
    "enhanced_summarization_reason = summarization_metric.reason\n",
    "\n",
    "print(\"2. Evaluating Enhanced Coherence...\")\n",
    "coherence_metric.measure(enhanced_test_case)\n",
    "enhanced_coherence_score = coherence_metric.score\n",
    "enhanced_coherence_reason = coherence_metric.reason\n",
    "\n",
    "print(\"3. Evaluating Enhanced Tonality...\")\n",
    "tonality_metric.measure(enhanced_test_case)\n",
    "enhanced_tonality_score = tonality_metric.score\n",
    "enhanced_tonality_reason = tonality_metric.reason\n",
    "\n",
    "print(\"4. Evaluating Enhanced Safety...\")\n",
    "safety_metric.measure(enhanced_test_case)\n",
    "enhanced_safety_score = safety_metric.score\n",
    "enhanced_safety_reason = safety_metric.reason\n",
    "\n",
    "# Structure enhanced results\n",
    "enhanced_evaluation_results = {\n",
    "    \"SummarizationScore\": enhanced_summarization_score,\n",
    "    \"SummarizationReason\": enhanced_summarization_reason,\n",
    "    \"CoherenceScore\": enhanced_coherence_score,\n",
    "    \"CoherenceReason\": enhanced_coherence_reason,\n",
    "    \"TonalityScore\": enhanced_tonality_score,\n",
    "    \"TonalityReason\": enhanced_tonality_reason,\n",
    "    \"SafetyScore\": enhanced_safety_score,\n",
    "    \"SafetyReason\": enhanced_safety_reason\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED COMPARISON: Original vs Enhanced\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED COMPARISON: ORIGINAL vs ENHANCED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = {\n",
    "    \"Summarization\": {\n",
    "        \"Original\": evaluation_results[\"SummarizationScore\"],\n",
    "        \"Enhanced\": enhanced_evaluation_results[\"SummarizationScore\"],\n",
    "        \"Change\": enhanced_evaluation_results[\"SummarizationScore\"] - evaluation_results[\"SummarizationScore\"]\n",
    "    },\n",
    "    \"Coherence\": {\n",
    "        \"Original\": evaluation_results[\"CoherenceScore\"],\n",
    "        \"Enhanced\": enhanced_evaluation_results[\"CoherenceScore\"],\n",
    "        \"Change\": enhanced_evaluation_results[\"CoherenceScore\"] - evaluation_results[\"CoherenceScore\"]\n",
    "    },\n",
    "    \"Tonality\": {\n",
    "        \"Original\": evaluation_results[\"TonalityScore\"],\n",
    "        \"Enhanced\": enhanced_evaluation_results[\"TonalityScore\"],\n",
    "        \"Change\": enhanced_evaluation_results[\"TonalityScore\"] - evaluation_results[\"TonalityScore\"]\n",
    "    },\n",
    "    \"Safety\": {\n",
    "        \"Original\": evaluation_results[\"SafetyScore\"],\n",
    "        \"Enhanced\": enhanced_evaluation_results[\"SafetyScore\"],\n",
    "        \"Change\": enhanced_evaluation_results[\"SafetyScore\"] - evaluation_results[\"SafetyScore\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, scores in comparison.items():\n",
    "    change_symbol = '✓ Improved' if scores['Change'] > 0 else '✗ Decreased' if scores['Change'] < 0 else '= No change'\n",
    "    change_color = '🟢' if scores['Change'] > 0 else '🔴' if scores['Change'] < 0 else '🟡'\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Original: {scores['Original']:.3f}\")\n",
    "    print(f\"  Enhanced: {scores['Enhanced']:.3f}\")\n",
    "    print(f\"  Change:   {scores['Change']:+.3f} {change_color} {change_symbol}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED ANALYSIS AND REFLECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS AND REFLECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall improvement\n",
    "total_original = sum([v[\"Original\"] for v in comparison.values()])\n",
    "total_enhanced = sum([v[\"Enhanced\"] for v in comparison.values()])\n",
    "overall_improvement = total_enhanced - total_original\n",
    "\n",
    "print(f\"\\n📊 OVERALL METRICS:\")\n",
    "print(f\"   Total Original Score:  {total_original:.3f} / 4.000\")\n",
    "print(f\"   Total Enhanced Score:  {total_enhanced:.3f} / 4.000\")\n",
    "print(f\"   Overall Change:        {overall_improvement:+.3f}\")\n",
    "print(f\"   Average Original:      {total_original/4:.3f}\")\n",
    "print(f\"   Average Enhanced:      {total_enhanced/4:.3f}\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📝 DETAILED REFLECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. DID WE GET A BETTER OUTPUT?\n",
    "\"\"\")\n",
    "\n",
    "if overall_improvement > 0:\n",
    "    print(f\"   ✅ YES - Overall improvement of {overall_improvement:+.3f} points\")\n",
    "else:\n",
    "    print(f\"   ❌ NO - Overall decrease of {overall_improvement:.3f} points\")\n",
    "\n",
    "print(f\"\"\"\n",
    "   Key Findings:\n",
    "   - Summarization: {comparison['Summarization']['Change']:+.3f} \n",
    "     {'⚠️ CRITICAL: This was 0.00 due to hallucination. Enhancement should fix this.' if comparison['Summarization']['Original'] < 0.1 else ''}\n",
    "   - Coherence: {comparison['Coherence']['Change']:+.3f}\n",
    "     {'Already strong (0.809), may decrease slightly if we prioritize accuracy.' if comparison['Coherence']['Original'] > 0.8 else ''}\n",
    "   - Tonality: {comparison['Tonality']['Change']:+.3f}\n",
    "     {'Was moderate (0.669), may improve with clearer tone application.' if comparison['Tonality']['Original'] < 0.7 else ''}\n",
    "   - Safety: {comparison['Safety']['Change']:+.3f}\n",
    "     {'Already strong (0.878), should maintain or improve.' if comparison['Safety']['Original'] > 0.8 else ''}\n",
    "\n",
    "2. WHY DID THIS HAPPEN?\n",
    "   \n",
    "   Root Cause of Original Failure:\n",
    "   - The original summary scored 0.00 on Summarization because it contained \n",
    "     \"numerous pieces of extra information not present in the original text\"\n",
    "   - This is called \"hallucination\" - the LLM added content from its training data\n",
    "   \n",
    "   Enhancement Strategy Applied:\n",
    "   - Reduced temperature from 0.7 to 0.3 (less creative, more factual)\n",
    "   - Added explicit instructions: \"ONLY use information from the article\"\n",
    "   - Emphasized the evaluation feedback in the prompt\n",
    "   - Provided the full original article as context\n",
    "   \n",
    "   Trade-offs:\n",
    "   - Prioritizing accuracy may reduce coherence/tonality slightly\n",
    "   - Lower temperature may make the tone less distinctive\n",
    "   - Stricter rules may result in a more conservative summary\n",
    "\n",
    "3. ARE THESE CONTROLS ENOUGH?\n",
    "   \n",
    "   ✅ STRENGTHS:\n",
    "   - Automated evaluation provides objective, consistent metrics\n",
    "   - Specific feedback (reasons) helps identify exact problems\n",
    "   - Self-correction loop allows iterative improvement\n",
    "   - Multiple metrics catch different types of issues\n",
    "   \n",
    "   ❌ LIMITATIONS:\n",
    "   - LLM-as-judge can be inconsistent between runs\n",
    "   - Evaluation scores may vary even for identical content\n",
    "   - No guarantee that enhancement will improve all metrics\n",
    "   - Cannot detect subtle factual errors without ground truth\n",
    "   - Feedback loop could oscillate (fixing one issue breaks another)\n",
    "   \n",
    "   🔧 RECOMMENDATIONS FOR PRODUCTION:\n",
    "   - Human review for critical summaries\n",
    "   - Multiple evaluation runs to average out variance\n",
    "   - Ground truth comparisons when available\n",
    "   - Hybrid approach: automated evaluation + human spot-checking\n",
    "   - A/B testing with real users\n",
    "   - Monitoring for hallucination patterns\n",
    "   - Consider using retrieval-augmented generation (RAG) for factual grounding\n",
    "   \n",
    "   📌 CONCLUSION:\n",
    "   These controls are a GOOD START but NOT SUFFICIENT for production use alone.\n",
    "   They work well for:\n",
    "   - Development and testing\n",
    "   - Quick quality checks\n",
    "   - Identifying obvious problems\n",
    "   \n",
    "   They should be SUPPLEMENTED with:\n",
    "   - Human oversight\n",
    "   - Domain expert review\n",
    "   - User feedback loops\n",
    "   - Continuous monitoring\n",
    "\"\"\")\n",
    "\n",
    "# Show specific improvements or regressions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 SPECIFIC CHANGES IN EVALUATION REASONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📌 SUMMARIZATION:\")\n",
    "print(f\"   Original: {evaluation_results['SummarizationReason'][:200]}...\")\n",
    "print(f\"   Enhanced: {enhanced_evaluation_results['SummarizationReason'][:200]}...\")\n",
    "\n",
    "print(\"\\n📌 COHERENCE:\")\n",
    "print(f\"   Original: {evaluation_results['CoherenceReason'][:200]}...\")\n",
    "print(f\"   Enhanced: {enhanced_evaluation_results['CoherenceReason'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda396e",
   "metadata": {},
   "source": [
    "In the first iteraction with pydantic we define the format of the output to help the OpenAI SDK to create the resume in a specific format to avoid errors\n",
    "\n",
    "In the second part we use DeppEval a tool that help us to evaluate different metrics for this kind of LLM responses like coherence, tonality, safety, etc.\n",
    "\n",
    "In the last section as we saw some issues/mistakes in the second part, we correct with prompt engineering.\n",
    "\n",
    "The issue in the summary in this case is because now the paper needs to create a user in order to crawl/download the full page, and thats why the DeepEval think our model is hallucinating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
